{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log2 as log\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "eps = np.finfo(float).eps\n",
    "import pprint\n",
    "\n",
    "dataset = pd.read_csv('Bank_dataset.csv',usecols=['job', 'marital', 'education', 'housing', 'y'], delimiter=';')\n",
    "dataPercentage = int(input(\"Enter the percentage of data to be read: \"))\n",
    "# calculate the number of rows to be read\n",
    "rows = int((dataPercentage/100)*len(dataset))\n",
    "# read only the required number of rows\n",
    "dataset = dataset[:rows]\n",
    "# divide the dataset into training and testing sets in 80:20 ratio\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the prior probabilities\n",
    "def priorProbabilities(train):\n",
    "    yes = train[train['y'] == 'yes']\n",
    "    no = train[train['y'] == 'no']\n",
    "    yesProb = len(yes) / len(train)\n",
    "    noProb = len(no) / len(train)\n",
    "    return yesProb, noProb\n",
    "\n",
    "#create a dictionary of conditional probabilities\n",
    "def conditionalProbabilities(train):\n",
    "    # get unique values for each feature\n",
    "    features = {}\n",
    "    for feature in train:\n",
    "        features[feature] = train[feature].unique()\n",
    "\n",
    "    # initialize conditional probabilities dictionary\n",
    "    conditionalProb = {}\n",
    "    for feature in features:\n",
    "        conditionalProb[feature] = {}\n",
    "        for value in features[feature]:\n",
    "            conditionalProb[feature][value] = {\"yes\": {\"count\": 0, \"prob\": 0}, \"no\": {\"count\": 0, \"prob\": 0}}\n",
    "\n",
    "    # calculate the conditional probabilities\n",
    "    for feature in features:\n",
    "        for value in features[feature]:\n",
    "            yes = train[train['y'] == 'yes']\n",
    "            no = train[train['y'] == 'no']\n",
    "            for i in range(len(yes)):\n",
    "                if yes[feature].iloc[i] == value:\n",
    "                    conditionalProb[feature][value][\"yes\"][\"count\"] += 1\n",
    "            for i in range(len(no)):\n",
    "                if no[feature].iloc[i] == value:\n",
    "                    conditionalProb[feature][value][\"no\"][\"count\"] += 1\n",
    "\n",
    "    # calculate probabilities of each value given the output class\n",
    "    for feature in features:\n",
    "        for value in features[feature]:\n",
    "            yes_count = sum([conditionalProb[feature][v][\"yes\"][\"count\"] for v in features[feature]])\n",
    "            no_count = sum([conditionalProb[feature][v][\"no\"][\"count\"] for v in features[feature]])\n",
    "            conditionalProb[feature][value][\"yes\"][\"prob\"] = conditionalProb[feature][value][\"yes\"][\"count\"] / yes_count\n",
    "            conditionalProb[feature][value][\"no\"][\"prob\"] = conditionalProb[feature][value][\"no\"][\"count\"] / no_count\n",
    "    return conditionalProb\n",
    "\n",
    "# predict the output class for the test set\n",
    "def naive_bayes():\n",
    "    yesProb, noProb = priorProbabilities(train)\n",
    "    condProb = conditionalProbabilities(train)\n",
    "    predictions = []\n",
    "    for i in range(len(test)):\n",
    "        testFeatures = test.iloc[i, :-1]  # get the feature values for the test example\n",
    "        yesPosterior = yesProb  # start with the prior probability for 'yes'\n",
    "        noPosterior = noProb    # start with the prior probability for 'no'\n",
    "        for feature, value in zip(testFeatures.index, testFeatures.values):\n",
    "            yesPosterior *= condProb[feature][value]['yes']['prob']  # multiply by the conditional probability for 'yes'\n",
    "            noPosterior *= condProb[feature][value]['no']['prob']    # multiply by the conditional probability for 'no'\n",
    "        if yesPosterior > noPosterior:\n",
    "            predictions.append('yes')\n",
    "        else:\n",
    "            predictions.append('no')\n",
    "    return predictions\n",
    "\n",
    "NB_Predictions = naive_bayes()\n",
    "# print(NB_Predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       no\n",
      "1       no\n",
      "2       no\n",
      "3       no\n",
      "4       no\n",
      "        ..\n",
      "4063    no\n",
      "4064    no\n",
      "4065    no\n",
      "4066    no\n",
      "4067    no\n",
      "Length: 4068, dtype: object\n",
      "_________________\n"
     ]
    }
   ],
   "source": [
    "def find_entropy(dataset):\n",
    "# calculate entropy of the dataset as a whole\n",
    "    Class = dataset.keys()[-1] #output class -> yes or no\n",
    "    entropy = 0\n",
    "    values = dataset[Class].unique()\n",
    "    for value in values:\n",
    "        fraction = dataset[Class].value_counts()[value]/len(dataset[Class])\n",
    "        entropy += -fraction*np.log2(fraction)\n",
    "    return entropy\n",
    "\n",
    "# calculate entropy of a specific attribute\n",
    "def find_entropy_attribute(dataset, attribute):\n",
    "    Class = dataset.keys()[-1] #output class \n",
    "    target_variables = dataset[Class].unique()  # This gives all 'Yes' and 'No'\n",
    "    # This gives different features in that attribute (like 'Hot','Cold' in Temperature)\n",
    "    variables = dataset[attribute].unique()\n",
    "    entropy2 = 0\n",
    "    # calculate entropy for each value in the attribute\n",
    "    for variable in variables:\n",
    "        entropy = 0\n",
    "        for target_variable in target_variables:\n",
    "            num = len(dataset[attribute][dataset[attribute] == variable][dataset[Class] == target_variable])\n",
    "            den = len(dataset[attribute][dataset[attribute] == variable])\n",
    "            fraction = num/(den+eps)\n",
    "            entropy += -fraction*log(fraction+eps) #enropy for one feature value\n",
    "        fraction2 = den/len(dataset)\n",
    "        entropy2 += -fraction2*entropy #entropy for the whole attribute\n",
    "    return abs(entropy2)\n",
    "\n",
    "\n",
    "def find_winner(dataset):\n",
    "    IG = []\n",
    "    for key in dataset.keys()[:-1]:\n",
    "        #         Entropy_att.append(find_entropy_attribute(dataset,key))\n",
    "        IG.append(find_entropy(dataset) - find_entropy_attribute(dataset, key))\n",
    "    return dataset.keys()[:-1][np.argmax(IG)]\n",
    "\n",
    "#subtable is the subset of the data where the attribute value is the same\n",
    "def get_subtable(dataset, node, value):\n",
    "    return dataset[dataset[node] == value].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def buildTree(dataset, tree=None, max_depth=2):\n",
    "    Class = dataset.keys()[-1]\n",
    "\n",
    "    # Here we build our decision tree\n",
    "\n",
    "    # Check if all instances in the subtable have the same class label\n",
    "    if len(dataset[Class].unique()) == 1:\n",
    "        return dataset[Class].iloc[0]\n",
    "\n",
    "    # Check if maximum depth is reached\n",
    "    if max_depth == 0:\n",
    "        # Determine the majority class label in the current subset\n",
    "        majority_class = dataset[Class].value_counts().idxmax()\n",
    "        return majority_class\n",
    "\n",
    "    # Get attribute with maximum information gain\n",
    "    node = find_winner(dataset)\n",
    "\n",
    "    # Get distinct values of that attribute\n",
    "    attValues = np.unique(dataset[node])\n",
    "\n",
    "    # Create an empty dictionary to represent the tree\n",
    "    if tree is None:\n",
    "        tree = {}\n",
    "        tree[node] = {}  # This is the root node\n",
    "\n",
    "    # Iterate over the attribute values and recursively build the tree\n",
    "    for value in attValues:\n",
    "        # Get subtable for the current value of the attribute\n",
    "        subtable = get_subtable(dataset, node, value)\n",
    "        # Check if the subset is empty\n",
    "        if subtable.empty:\n",
    "            # Determine the majority class label in the current subset\n",
    "            majority_class = dataset[Class].value_counts().idxmax()\n",
    "            tree[node][value] = majority_class\n",
    "        else:\n",
    "            # Decrement the maximum depth by 1\n",
    "            sub_max_depth = max_depth - 1\n",
    "            # Recursively build the tree using the subtable\n",
    "            tree[node][value] = buildTree(subtable, max_depth=sub_max_depth)\n",
    "\n",
    "    return tree\n",
    "\n",
    "def predict(inst,tree):\n",
    "    #This function is used to predict for any input variable     \n",
    "    #Recursively we go through the tree that we built earlier\n",
    "    \n",
    "    for nodes in tree.keys(): # job, marital, education, default, housing, loan, contact, month, day_of_week, poutcome  \n",
    "        value = inst[nodes] #if node is job, value could be admin\n",
    "        tree = tree[nodes][value] #get the decision or sub-tree that corresponds to that value of the node\n",
    "        prediction = 0\n",
    "\n",
    "        if type(tree) is dict:\n",
    "            prediction = predict(inst, tree)\n",
    "        else:\n",
    "            prediction = tree\n",
    "            break;                            \n",
    "        \n",
    "    return prediction\n",
    "\n",
    "#build our tree and call predict to test it\n",
    "tree = buildTree(dataset, max_depth=5)\n",
    "# print the tree\n",
    "# pprint.pprint(tree)\n",
    "\n",
    "#predict for the whole test data\n",
    "DS_Predictions = dataset.apply(predict, args=(tree,), axis=1)\n",
    "print(DS_Predictions)\n",
    "print(\"_________________\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuraccy when Reading 90 % of the DataSet\n",
      "Naive Bayse Accuracy:  88.6977886977887 %\n",
      "Decision Tree Accuracy:  87.83783783783784 %\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy for each model \n",
    "def get_accuracy(predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(test)):\n",
    "        if test['y'].iloc[i] == predictions[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(test)\n",
    "    return accuracy\n",
    "# print the accuracy percentage\n",
    "print(\"Model Accuraccy when Reading\", dataPercentage,\"% of the DataSet\")\n",
    "print(\"Naive Bayse Accuracy: \", get_accuracy(NB_Predictions)*100,\"%\")\n",
    "print(\"Decision Tree Accuracy: \", get_accuracy(DS_Predictions)*100,\"%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
